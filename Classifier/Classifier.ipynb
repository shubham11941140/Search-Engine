{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"aqVxtvPO9Exg"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from tqdm.notebook import tqdm\n","from sklearn.metrics import classification_report\n","import random"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"PPLIWXL39Exm","outputId":"ac10de8c-106a-4ca9-9b09-380ee145ef33"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to C:\\Users\\shubham\n","[nltk_data]     gupta\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to C:\\Users\\shubham\n","[nltk_data]     gupta\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to C:\\Users\\shubham\n","[nltk_data]     gupta\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","\n","# Downloading NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Creating a TF-IDF Model Class\n","class TfIdfModel:\n","    \n","    # TODO. Add getter setter for all variables\n","    # TODO. Add update Data and Recalculate TF-IDF Functionality\n","    # TODO. Add everything else in the Class\n","    \n","    # Constructor\n","    def __init__(self, docs_data):\n","        \n","        self.number_of_docs = len(docs_data)\n","        self.document_collection = docs_data\n","        self.tokenized_collection = dict()\n","        self.vocabulary = set()\n","        self.inverted_index = dict()\n","        self.document_frequency = dict()\n","        self.term_document_frequency = dict()\n","        self.max_freq_in_doc = dict()\n","        self.tf_idf_scores = dict()\n","        \n","        # Tokenize the documents to build the tokenized collection\n","        print(\"Tokenizing the documents...\")\n","        self.tokenize_documents()\n","        print(\"Tokenization complete!\")\n","        \n","        # Create the vocabulary\n","        print(\"Creating the vocabulary...\")\n","        self.create_vocabulary()\n","        print(\"Vocabulary created!\")\n","        \n","        # Create the inverted index\n","        print(\"Creating the inverted index...\")\n","        self.create_inverted_index()\n","        print(\"Inverted index created!\")\n","        \n","                \n","        # Calculate the document frequency\n","        print(\"Calculating the document frequency...\")\n","        for term in self.vocabulary:\n","            self.document_frequency[term] = len(self.inverted_index[term])\n","        print(\"Document frequency calculated!\")\n","        \n","        \n","        # Calculate the term document frequency\n","        print(\"Calculating the term document frequency...\")\n","        for word in self.vocabulary:\n","            self.term_document_frequency[word] = dict()\n","            \n","            for doc in self.inverted_index[word]:\n","                self.term_document_frequency[word][doc[0]] = doc[1]\n","                       \n","            for doc in set(list(self.document_collection.keys())) - set(self.term_document_frequency[word].keys()):\n","                self.term_document_frequency[word][doc] = 0\n","        print(\"Term document frequency calculated!\")\n","        \n","        \n","        # Calculate the max frequency in a document\n","        print(\"Calculating the max frequency in a document...\")\n","        for doc_id in self.document_collection:\n","            self.max_freq_in_doc[doc_id] = 0\n","            for word in self.tokenized_collection[doc_id]:\n","                if self.term_document_frequency[word][doc_id] > self.max_freq_in_doc[doc_id]:\n","                    self.max_freq_in_doc[doc_id] = self.term_document_frequency[word][doc_id]\n","        print(\"Max frequency in a document calculated!\")                    \n","\n","        \n","        # Calculate the TF-IDF Scores\n","        print(\"Calculating the TF-IDF scores...\")\n","        self.calculate_tf_idf_scores()\n","        print(\"TF-IDF scores calculated!\")\n","        \n","    \n","    # Function to get a list of all documents\n","    # Input: None\n","    # Output: List of all documents : List[Str]\n","    def get_document_list(self):\n","        return list(self.document_collection.keys())\n","    \n","    def get_document_content(self, doc_id):\n","        try:\n","            return self.document_collection[doc_id]\n","        except KeyError:\n","            raise Exception(\"KeyError: Document ID not found\")\n","    \n","    \n","    \n","    # Function to get the tokens of a document\n","    # Input: Document ID : Str\n","    # Output: List of tokens : List[Str]\n","    def get_document_tokens(self, doc_id):\n","        try:\n","            return self.tokenized_collection[doc_id]\n","        except KeyError:\n","            raise Exception(\"KeyError: Document ID not found\")\n","    \n","    \n","    # Function to extract keywords from a document\n","    # Input: Document ID : Str\n","    # Output: List of keywords : List[Str]\n","    def extract_keywords(self, text):\n","        tokens = nltk.word_tokenize(text)\n","        \n","        # Remove stop words\n","        tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words('english')]\n","        \n","        # Stem the words\n","        tokens = [nltk.PorterStemmer().stem(word) for word in tokens]\n","        \n","        # Remove Punctuation\n","        tokens = [word for word in tokens if word.isalpha()]\n","        return tokens\n","\n","\n","    # Function to tokenize the documents\n","    # Input: None\n","    # Output: None (Updates the tokenized collection dictionary)\n","    def tokenize_documents(self):\n","        for doc_id in self.document_collection:\n","            \n","            # Tokenize the document\n","            tokens = self.extract_keywords(self.get_document_content(doc_id))\n","            \n","            # Add the tokens to the collection\n","            self.tokenized_collection[doc_id] = tokens\n","    \n","    \n","    # Function to Create the vocabulary\n","    # Input: None\n","    # Output: None (Updates the vocabulary set)\n","    def create_vocabulary(self):\n","        for doc_id in self.tokenized_collection:\n","            self.vocabulary.update(self.tokenized_collection[doc_id])\n","    \n","    \n","    # Function to get the vocabulary\n","    # Input: None\n","    # Output: List of vocabulary : List[Str]\n","    def get_vocabulary(self):\n","        return list(self.vocabulary)\n","    \n","    \n","    # Function to create the inverted index\n","    # Input: None\n","    # Output: None (Updates the inverted index dictionary)\n","    def create_inverted_index(self):\n","        \n","        for word in self.vocabulary:\n","            self.inverted_index[word] = []\n","\n","        for doc_id in self.tokenized_collection:\n","\n","            word_frequency = dict()\n","            for word in self.tokenized_collection[doc_id]:\n","                if word in word_frequency:\n","                    word_frequency[word] += 1\n","                else:\n","                    word_frequency[word] = 1\n","            \n","            for word in word_frequency:\n","                self.inverted_index[word].append((doc_id, word_frequency[word]))\n","            \n","    # Get the inverted index\n","    # Input: None\n","    # Output: Inverted index dictionary : Dict[Str, List[Tuple[Str, Int]]]\n","    def get_inverted_index(self):\n","        return self.inverted_index\n","    \n","    \n","    # Get the postings list of a word\n","    # Input: Word : Str\n","    # Output: Postings list : List[Tuple[Str, Int]]\n","    def get_postings_list(self, word):\n","        try:\n","            return self.inverted_index[word]\n","        except KeyError:\n","            raise Exception(\"KeyError: Word not found\")\n","    \n","    \n","    # Calculate the Tf-IDf scores of the documents\n","    # Input: None\n","    # Output: None (Updates the tf-idf scores dictionary)\n","    def calculate_tf_idf_scores(self):\n","        for doc in self.document_collection:\n","            self.tf_idf_scores[doc] = {}\n","            for word in self.tokenized_collection[doc]:\n","                self.tf_idf_scores[doc][word] = 0.5 + 0.5*(self.term_document_frequency[word][doc]/self.max_freq_in_doc[doc]) * np.log(self.number_of_docs/self.document_frequency[word] + 1)\n","    \n","    # Get Tf-Idf Dataframe\n","    # Input: None\n","    # Output: Tf-Idf Dataframe : Dataframe\n","    def get_tf_idf_dataframe(self):\n","        return pd.DataFrame.from_dict(tf_idf_model.tf_idf_scores)"]},{"cell_type":"markdown","metadata":{"id":"4ObMWnD29Exv"},"source":["READING THE DATA FROM THE DATASET"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3aP_VVK9Exx","outputId":"43f6d2ac-41e9-4b12-f2fa-dcab738e7d5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["# of Documents: 6377\n"]}],"source":["# Path variables\n","docs_data = {}\n","DocumentsPath = './alldocs/'\n","Total_Docs = len(os.listdir(DocumentsPath))\n","print(\"# of Documents:\", Total_Docs)\n","output_path = \"./output.txt\"\n","queries_path = \"./query.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rfOrW0k9Exy"},"outputs":[],"source":["with open(output_path, 'r') as f:\n","    \n","    # Read the file and store it in a list\n","    try:\n","        output_data = f.read().splitlines()\n","    except:\n","        print(\"Error in reading file:\", output_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QsaJMyt9Exy","outputId":"52d445b8-97b9-4c28-ec89-cfd9a7372c9a"},"outputs":[{"data":{"text/plain":["dict_keys(['701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '760', '761', '762', '763', '764', '765', '766', '767', '768', '769', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '780', '781', '782', '783', '784', '785', '786', '787', '788', '789', '790', '791', '792', '793', '794', '795', '796', '797', '798', '799', '800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '813', '814', '815', '816', '817', '818', '819', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '830', '831', '832', '833', '834', '835', '836', '837', '838', '839', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850'])"]},"execution_count":240,"metadata":{},"output_type":"execute_result"}],"source":["# Indexing Data\n","\n","inv_classes_dict = {}\n","for line in output_data:\n","    line_list = line.split(\" \")\n","    query_id = line_list[0]\n","    doc_id = line_list[1]\n","    if doc_id in inv_classes_dict.keys():\n","        inv_classes_dict[doc_id] = None\n","    else:\n","        inv_classes_dict[doc_id] = query_id\n","\n","classes_dict = {}\n","for doc_id in inv_classes_dict.keys():\n","    query_id = inv_classes_dict[doc_id]\n","    if query_id is not None:\n","        if query_id not in classes_dict.keys():\n","            classes_dict[query_id] = []\n","        else :\n","            classes_dict[query_id].append(doc_id)\n","classes_dict.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAGupf3R9Exz","outputId":"e5d387bb-4e0c-48d0-a5ac-a57e4ca9d3f4"},"outputs":[{"data":{"text/plain":["(5050, 2246)"]},"execution_count":241,"metadata":{},"output_type":"execute_result"}],"source":["# Split data into 70:30 ratio per class for train and test respectively\n","train_set = []\n","test_set = []\n","for query_id in classes_dict.keys():\n","    l = len(classes_dict[query_id])\n","    temp_list = classes_dict[query_id].copy()\n","    random.shuffle(temp_list)\n","    train_set.extend(temp_list[:int(.7*l)])\n","    test_set.extend(temp_list[int(.7*l):])\n","len(train_set), len(test_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6WslnJr9Ex2"},"outputs":[],"source":["# Read all docs into a dataframe\n","\n","Files_Not_Read = []\n","\n","Number_of_Docs_To_Read = 100\n","\n","DOCS = os.listdir(DocumentsPath)[:Number_of_Docs_To_Read]\n","\n","# Iterate over all files in the directory\n","for file in DOCS:\n","    \n","    # Read the file\n","    with open(DocumentsPath + file, 'r') as f:\n","        \n","        # Read the file and store it in a list\n","        try:\n","            docs_data[file] = f.read()\n","        except:\n","            print(\"Error in reading file:\", file)\n","            Files_Not_Read.append(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaKrZEw19Ex3","outputId":"629eafe0-1faa-4b70-a7e2-329de6fcaa84"},"outputs":[{"name":"stdout","output_type":"stream","text":["# of Documents Read: 100\n","[]\n"]}],"source":["Total_Docs = len(docs_data)\n","print(\"# of Documents Read:\", Total_Docs)\n","print(Files_Not_Read)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJ8lEeS89Ex3","outputId":"5c94a6d7-09fe-4535-e95f-4ac9a59d5586"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing the documents...\n","Tokenization complete!\n","Creating the vocabulary...\n","Vocabulary created!\n","Creating the inverted index...\n","Inverted index created!\n","Calculating the document frequency...\n","Document frequency calculated!\n","Calculating the term document frequency...\n","Term document frequency calculated!\n","Calculating the max frequency in a document...\n","Max frequency in a document calculated!\n","Calculating the TF-IDF scores...\n","TF-IDF scores calculated!\n","100\n","['GX000-01-10544170', 'GX000-09-2703409', 'GX000-10-4524900', 'GX000-14-10770491', 'GX000-14-16748010', 'GX000-14-5445022', 'GX000-16-0145015', 'GX000-16-4063715', 'GX000-21-2440731', 'GX000-22-12322384', 'GX000-26-8535353', 'GX000-29-8328421', 'GX000-31-2605300', 'GX000-33-0298602', 'GX000-34-9679963', 'GX000-36-2289681', 'GX000-37-9159393', 'GX000-38-10952535', 'GX000-39-14470745', 'GX000-43-1993135', 'GX000-43-4226578', 'GX000-46-4667412', 'GX000-46-8625035', 'GX000-47-11266472', 'GX000-47-16664622', 'GX000-48-10208090', 'GX000-49-12224349', 'GX000-52-8772594', 'GX000-53-0788254', 'GX000-55-3026780', 'GX000-56-16493385', 'GX000-60-10029828', 'GX000-62-7241305', 'GX000-63-3474016', 'GX000-63-3636455', 'GX000-64-5866723', 'GX000-65-2909487', 'GX000-66-4641344', 'GX000-66-9648548', 'GX000-69-6846304', 'GX000-70-12794819', 'GX000-72-9170474', 'GX000-75-11356764', 'GX000-75-12829778', 'GX000-75-6612197', 'GX000-76-13196608', 'GX000-76-13395038', 'GX000-76-2231612', 'GX000-77-6847828', 'GX000-80-10421013', 'GX000-81-0219756', 'GX000-82-0419685', 'GX000-85-1129126', 'GX000-88-5542873', 'GX000-90-7867636', 'GX000-93-5908540', 'GX000-93-8422909', 'GX000-95-8621250', 'GX000-98-11305946', 'GX000-98-16312886', 'GX001-06-14795789', 'GX001-06-8404221', 'GX001-07-10752136', 'GX001-08-5834860', 'GX001-08-7866949', 'GX001-10-2816967', 'GX001-10-4196524', 'GX001-10-9697910', 'GX001-12-8380564', 'GX001-14-1170888', 'GX001-16-14126351', 'GX001-17-2932555', 'GX001-19-3646687', 'GX001-21-2455320', 'GX001-21-4314384', 'GX001-22-5770689', 'GX001-23-13679550', 'GX001-27-3364162', 'GX001-27-5701295', 'GX001-28-1213107', 'GX001-28-14093471', 'GX001-28-5148808', 'GX001-29-1560403', 'GX001-30-5435939', 'GX001-30-5929274', 'GX001-32-14918699', 'GX001-34-14403731', 'GX001-35-0432171', 'GX001-35-14901241', 'GX001-35-8607922', 'GX001-37-11210821', 'GX001-37-1534868', 'GX001-37-16595181', 'GX001-37-2825497', 'GX001-39-3118899', 'GX001-41-4669647', 'GX001-43-0085597', 'GX001-43-11803200', 'GX001-44-13913188', 'GX001-45-9859222']\n"]}],"source":["# Create a TF-IDF Model\n","tf_idf_model = TfIdfModel(docs_data)\n","\n","print(tf_idf_model.number_of_docs)\n","print(tf_idf_model.get_document_list())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfrzUkvr9Ex4","outputId":"3eae2640-98e4-4ca5-fba3-be090168fa0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary Size: 16226\n"]}],"source":["print(\"Vocabulary Size:\", len(tf_idf_model.get_vocabulary()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnT_ghxd9Ex5","outputId":"e2c7bfed-6a59-4d07-e6e5-28526e48081c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens of first document: ['link', 'nation', 'cancer', 'institut', 'center', 'cancer', 'research', 'link', 'contact', 'ccr', 'link', 'ccr', 'homepag', 'link', 'nci', 'home', 'link', 'nih', 'home', 'search', 'compar', 'oncolog', 'program', 'home', 'introduct', 'specif', 'aim', 'time', 'for', 'implement', 'background', 'the', 'ccr', 'compar', 'oncolog', 'program', 'compliment', 'number', 'new', 'ccr', 'initi', 'design', 'improv', 'translat', 'process', 'program', 'announc', 'compar', 'oncolog', 'refer', 'studi', 'natur', 'develop', 'cancer', 'anim', 'model', 'human', 'diseas', 'a', 'signific', 'group', 'natur', 'occur', 'cancer', 'develop', 'pet', 'anim', 'primarili', 'cat', 'dog', 'these', 'larg', 'anim', 'cancer', 'share', 'mani', 'featur', 'human', 'cancer', 'includ', 'tumor', 'histolog', 'genet', 'respons', 'convent', 'therapi', 'biolog', 'behavior', 'exampl', 'model', 'includ', 'osteosarcoma', 'lymphoma', 'breast', 'cancer', 'head', 'neck', 'carcinoma', 'prostat', 'cancer', 'soft', 'tissu', 'sarcoma', 'melanoma', 'through', 'design', 'clinic', 'trial', 'includ', 'pet', 'anim', 'broader', 'understand', 'treatment', 'biolog', 'cancer', 'attain', 'mani', 'factor', 'contribut', 'valu', 'spontan', 'cancer', 'relev', 'model', 'human', 'cancer', 'they', 'share', 'tumor', 'biolog', 'human', 'counterpart', 'case', 'ident', 'tumor', 'histolog', 'respons', 'rate', 'convent', 'chemotherapi', 'pet', 'anim', 'share', 'mani', 'environment', 'risk', 'factor', 'human', 'owner', 'in', 'case', 'preval', 'cancer', 'suffici', 'clinic', 'trial', 'biolog', 'studi', 'initi', 'rapidli', 'complet', 'the', 'size', 'dog', 'cat', 'make', 'evalu', 'protocol', 'feasibl', 'includ', 'novel', 'approach', 'imag', 'cancer', 'patient', 'the', 'lack', 'gold', 'standard', 'treatment', 'allow', 'earli', 'human', 'evalu', 'novel', 'therapi', 'click', 'enlarg', 'for', 'pet', 'anim', 'pet', 'owner', 'clinic', 'trial', 'offer', 'opportun', 'exist', 'human', 'cancer', 'patient', 'choos', 'enter', 'clinic', 'trial', 'clinic', 'trial', 'design', 'evalu', 'newest', 'option', 'avail', 'treatment', 'cancer', 'the', 'major', 'novel', 'treatment', 'design', 'extens', 'understand', 'biolog', 'cancer', 'specif', 'target', 'cancer', 'cell', 'as', 'result', 'may', 'expect', 'cancer', 'treatment', 'better', 'toler', 'convent', 'form', 'chemotherapi', 'in', 'case', 'larg', 'part', 'cost', 'associ', 'treat', 'pet', 'anim', 'cancer', 'cover', 'particip', 'clinic', 'trial', 'at', 'time', 'open', 'clinic', 'trial', 'compar', 'oncolog', 'program', 'for', 'inform', 'clinic', 'trial', 'avail', 'pet', 'anim', 'unit', 'state', 'pleas', 'see', 'ccr', 'compar', 'oncolog', 'program', 'preclin', 'trail', 'undertaken', 'collabor', 'univers', 'veterinari', 'teach', 'hospit', 'other', 'collabor', 'group', 'includ', 'extramur', 'academ', 'institut', 'pharmaceut', 'industri', 'a', 'focu', 'trial', 'simultan', 'assess', 'agent', 'activ', 'valid', 'biolog', 'surrog', 'endpoint', 'directli', 'translat', 'futur', 'human', 'trial', 'the', 'ccr', 'compar', 'oncolog', 'program', 'initi', 'focu', 'effort', 'prostat', 'cancer', 'lymphoma', 'osteosarcoma', 'the', 'plan', 'outcom', 'collabor', 'effort', 'integr', 'compar', 'model', 'cancer', 'process', 'cancer', 'drug', 'develop', 'deliveri', 'the', 'newli', 'form', 'ccr', 'compar', 'oncolog', 'program', 'expect', 'becom', 'core', 'resourc', 'faculti', 'investig', 'within', 'ccr', 'interest', 'use', 'compar', 'model', 'cancer', 'the', 'initi', 'goal', 'ccr', 'compar', 'oncolog', 'program', 'includ', 'character', 'valid', 'compar', 'model', 'use', 'trial', 'design', 'implement', 'manag', 'trial', 'involv', 'pet', 'anim', 'evalu', 'novel', 'therapeut', 'strategi', 'develop', 'within', 'outsid', 'ccr', 'increas', 'awar', 'use', 'natur', 'occur', 'cancer', 'model', 'translat', 'biolog', 'cancer', 'research', 'for', 'inform', 'resourc', 'avail', 'compar', 'oncolog', 'program', 'contact', 'chand', 'khanna', 'khannac', 'link', 'nih', 'ccr', 'home', 'about', 'ccr', 'clinic', 'trial', 'research', 'employ', 'featur', 'initi', 'new', 'event', 'site', 'privaci', 'polici', 'access', 'center', 'cancer', 'research', 'nation', 'cancer', 'institut', 'link', 'firstgov']\n"]}],"source":["# Print tokens of first document\n","print(\"Tokens of first document:\", tf_idf_model.get_document_tokens(tf_idf_model.get_document_list()[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpF4MZCg9Ex5","outputId":"89221b63-92ea-4667-f8ce-96202e6902f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Inverted Index Size: 16226\n","Posting list of clinic: [('GX000-01-10544170', 9), ('GX000-14-10770491', 1), ('GX000-33-0298602', 64), ('GX000-39-14470745', 8), ('GX000-47-11266472', 1), ('GX000-53-0788254', 1), ('GX000-55-3026780', 1), ('GX000-76-13395038', 1), ('GX000-80-10421013', 6), ('GX000-93-5908540', 5), ('GX001-06-8404221', 6), ('GX001-10-2816967', 4), ('GX001-10-9697910', 2), ('GX001-12-8380564', 14), ('GX001-16-14126351', 23), ('GX001-19-3646687', 2), ('GX001-21-2455320', 1), ('GX001-28-1213107', 1), ('GX001-30-5435939', 4), ('GX001-37-11210821', 52), ('GX001-37-2825497', 3)]\n"]}],"source":["print(\"Inverted Index Size:\", len(tf_idf_model.get_inverted_index()))\n","print(\"Posting list of clinic:\", tf_idf_model.get_postings_list(\"clinic\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQknS5aS9Ex6","outputId":"adbf6e11-91ea-4a96-b4f4-8a4d51a8cde2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Term Document Frequency of clinic: {'GX000-01-10544170': 9, 'GX000-14-10770491': 1, 'GX000-33-0298602': 64, 'GX000-39-14470745': 8, 'GX000-47-11266472': 1, 'GX000-53-0788254': 1, 'GX000-55-3026780': 1, 'GX000-76-13395038': 1, 'GX000-80-10421013': 6, 'GX000-93-5908540': 5, 'GX001-06-8404221': 6, 'GX001-10-2816967': 4, 'GX001-10-9697910': 2, 'GX001-12-8380564': 14, 'GX001-16-14126351': 23, 'GX001-19-3646687': 2, 'GX001-21-2455320': 1, 'GX001-28-1213107': 1, 'GX001-30-5435939': 4, 'GX001-37-11210821': 52, 'GX001-37-2825497': 3, 'GX000-85-1129126': 0, 'GX001-41-4669647': 0, 'GX000-76-2231612': 0, 'GX000-90-7867636': 0, 'GX000-21-2440731': 0, 'GX000-46-4667412': 0, 'GX000-70-12794819': 0, 'GX000-09-2703409': 0, 'GX001-10-4196524': 0, 'GX001-21-4314384': 0, 'GX001-28-5148808': 0, 'GX000-63-3474016': 0, 'GX000-16-0145015': 0, 'GX000-38-10952535': 0, 'GX000-62-7241305': 0, 'GX001-35-0432171': 0, 'GX000-10-4524900': 0, 'GX000-66-9648548': 0, 'GX000-77-6847828': 0, 'GX001-37-1534868': 0, 'GX001-35-8607922': 0, 'GX001-14-1170888': 0, 'GX000-75-6612197': 0, 'GX000-29-8328421': 0, 'GX001-06-14795789': 0, 'GX001-45-9859222': 0, 'GX000-69-6846304': 0, 'GX000-43-4226578': 0, 'GX000-81-0219756': 0, 'GX001-08-5834860': 0, 'GX000-49-12224349': 0, 'GX001-28-14093471': 0, 'GX001-44-13913188': 0, 'GX000-48-10208090': 0, 'GX000-60-10029828': 0, 'GX000-95-8621250': 0, 'GX001-39-3118899': 0, 'GX000-82-0419685': 0, 'GX001-07-10752136': 0, 'GX001-22-5770689': 0, 'GX000-98-11305946': 0, 'GX000-93-8422909': 0, 'GX001-17-2932555': 0, 'GX001-27-5701295': 0, 'GX001-29-1560403': 0, 'GX001-32-14918699': 0, 'GX000-63-3636455': 0, 'GX000-75-11356764': 0, 'GX000-36-2289681': 0, 'GX000-46-8625035': 0, 'GX000-65-2909487': 0, 'GX001-08-7866949': 0, 'GX001-30-5929274': 0, 'GX000-43-1993135': 0, 'GX000-56-16493385': 0, 'GX000-37-9159393': 0, 'GX000-14-5445022': 0, 'GX000-31-2605300': 0, 'GX000-52-8772594': 0, 'GX001-43-11803200': 0, 'GX001-23-13679550': 0, 'GX000-22-12322384': 0, 'GX001-43-0085597': 0, 'GX000-72-9170474': 0, 'GX000-75-12829778': 0, 'GX000-64-5866723': 0, 'GX000-88-5542873': 0, 'GX001-34-14403731': 0, 'GX000-98-16312886': 0, 'GX000-14-16748010': 0, 'GX000-47-16664622': 0, 'GX000-26-8535353': 0, 'GX001-35-14901241': 0, 'GX000-76-13196608': 0, 'GX000-34-9679963': 0, 'GX000-66-4641344': 0, 'GX001-27-3364162': 0, 'GX001-37-16595181': 0, 'GX000-16-4063715': 0}\n","Document Frequency of clinic: 21\n","Maximum Frequency in Document with ID GX000-14-10770491: 43\n"]}],"source":["print(\"Term Document Frequency of clinic:\", tf_idf_model.term_document_frequency[\"clinic\"])\n","print(\"Document Frequency of clinic:\", tf_idf_model.document_frequency['clinic'])\n","print(\"Maximum Frequency in Document with ID GX000-14-10770491:\", tf_idf_model.max_freq_in_doc['GX000-14-10770491'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFPHHjZs9Ex6","outputId":"a68fb6c9-1230-46d5-ed4c-c5e72cb699b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF-IDF of GX000-14-10770491: {'doj': 0.5333916235451629, 'logo': 0.5162074600785497, 'comput': 0.6943963173394017, 'crime': 0.5473693471456056, 'intellectu': 0.5951250704211603, 'properti': 0.5530171774562005, 'section': 0.514567011261574, 'ccip': 0.5757696869307322, 'document': 0.515455069154418, 'star': 0.5230349008007742, 'rule': 0.5162074600785497, 'year': 0.5501646600784528, 'hialeah': 0.6371567081182904, 'florida': 0.6468079412958136, 'man': 1.46448029537077, 'sentenc': 1.1182279459979205, 'crimin': 0.6718610420617886, 'copyright': 0.9236936697235043, 'infring': 0.7536668544564272, 'novemb': 0.5706895699416006, 'attorney': 0.5416688248657687, 'gener': 0.5198834931630316, 'ashcroft': 0.5457189027060968, 'announc': 0.5170504310324817, 'oper': 0.5798349391986288, 'cyber': 0.537884843465366, 'sweep': 0.5317083568070534, 'five': 0.515455069154418, 'men': 0.5921396032030969, 'charg': 0.8453300903823064, 'new': 0.6067721625395353, 'hampshir': 0.5557650063441482, 'softwar': 1.3601432292144982, 'piraci': 1.1819271823765893, 'orang': 0.5519440051513278, 'counti': 0.5561431829918873, 'california': 0.6854608298530152, 'businessman': 0.6233529081242393, 'found': 0.5126215605126814, 'guilti': 1.1960681716886143, 'traffick': 0.7320114702225976, 'counterfeit': 1.014485170378541, 'compaq': 0.5914378054121936, 'compon': 0.5335165143887444, 'octob': 0.565910047431937, 'deputi': 0.526879475912953, 'assist': 0.5258633955310427, 'john': 0.5190956713401987, 'malcom': 0.5536641920562937, 'testimoni': 0.5290014337778247, 'privaci': 0.5173548438880837, 'legal': 0.5167582571943722, 'issu': 0.5121856808819693, 'relat': 0.5369818374828016, 'file': 0.5180040723308635, 'share': 0.515947433434125, 'over': 0.6041720621644218, 'internet': 0.7346156007212101, 'york': 0.6255745783208386, 'state': 0.5273529869336863, 'bar': 0.5218641030996454, 'associ': 0.5126215605126814, 'case': 0.5785589253155505, 'music': 0.6394125158603704, 'septemb': 0.6065516406079919, 'feder': 0.6109455124484049, 'juri': 0.537884843465366, 'convict': 0.5754338998110745, 'hacker': 0.537884843465366, 'boca': 0.541117636041413, 'raton': 0.541117636041413, 'violat': 0.5833376497315375, 'digit': 0.5806384277388592, 'millenium': 0.5333916235451629, 'act': 0.5991523497119214, 'connecticut': 0.5502892665407164, 'plead': 1.2346157179935846, 'lo': 0.6041720621644218, 'angel': 0.5897190972437926, 'steal': 0.8894156606390424, 'trade': 0.922072831225464, 'secret': 1.0447474921744642, 'pertain': 0.5537589518259062, 'card': 0.5806384277388592, 'technolog': 0.5329550237159685, 'gloucest': 0.5457189027060968, 'citi': 0.5149961340166375, 'jersey': 0.5208344124328843, 'admit': 0.5519440051513278, 'defraud': 0.537884843465366, 'ebay': 0.5822352720828262, 'bidder': 0.541117636041413, 'sterl': 0.5822352720828262, 'virginia': 0.6041720621644218, 'plea': 0.5354014236944584, 'creat': 0.5121856808819693, 'sell': 0.7842333402953896, 'pirat': 0.854014236944584, 'cd': 0.5757696869307322, 'august': 0.5837912859718608, 'onlin': 0.5381913426803975, 'leader': 0.5995917785683105, 'orlando': 0.5317083568070534, 'two': 0.568429947884539, 'resid': 0.5347096877761675, 'million': 0.5913299776639931, 'dollar': 0.6041720621644218, 'design': 0.5127745614961408, 'good': 0.5130931542192584, 'arrest': 0.6343973795647653, 'juli': 0.6369949664959896, 'brooklyn': 0.6136545303960982, 'undercov': 0.537884843465366, 'agent': 0.5381913426803975, 'queen': 0.541117636041413, 'damag': 0.5167582571943722, 'access': 0.5671491410011187, 'devic': 0.5796734228546484, 'fraud': 0.5502892665407164, 'fremont': 0.5914378054121936, 'packag': 0.5190956713401987, 'microsoft': 0.7467058162484785, 'massachusett': 0.5224297743109482, 'conspiraci': 0.6136545303960982, 'former': 0.6145740280411924, 'boe': 0.5457189027060968, 'manag': 0.5127745614961408, 'plot': 0.5536641920562937, 'lockhe': 0.5457189027060968, 'martin': 0.5317083568070534, 'june': 0.6034463489419456, 'hamilton': 0.537884843465366, 'nj': 0.5317083568070534, 'court': 0.5561431829918873, 'movi': 0.5667832470903259, 'hulk': 0.5457189027060968, 'post': 0.5173548438880837, 'razor': 0.5457189027060968, 'oldest': 0.5317083568070534, 'game': 0.5317083568070534, 'ring': 0.5754338998110745, 'concord': 0.5914378054121936, 'ohio': 0.565592309298936, 'manufactur': 0.5561431829918873, 'distribut': 0.6349652061497368, 'satellit': 0.6345786458656888, 'televis': 0.6121488715547407, 'may': 0.6182276084261215, 'grand': 0.5302638335516788, 'island': 0.5550529779859301, 'theft': 0.9274587655960891, 'download': 0.5208344124328843, 'engin': 0.5561431829918873, 'draw': 0.5251446332703582, 'data': 0.5141646213874174, 'make': 0.5210338668424919, 'illeg': 0.5975414473850807, 'intern': 0.5261863084385169, 'ringlead': 0.5457189027060968, 'with': 0.5309101383088358, 'san': 0.6280012451589597, 'francisco': 0.5224297743109482, 'electron': 0.5584891909108116, 'task': 0.5208344124328843, 'forc': 0.5136054796819797, 'bangkok': 0.5457189027060968, 'thailand': 0.5457189027060968, 'oakland': 0.5536641920562937, 'indict': 0.9956199317224177, 'proprietari': 0.541117636041413, 'databas': 0.5213335408598266, 'hollywood': 0.537884843465366, 'who': 0.565592309298936, 'use': 0.518858842237589, 'camcord': 0.541117636041413, 'threat': 0.5194963969702706, 'first': 0.5558408110574007, 'april': 0.6561935949927533, 'inform': 0.5266930406348839, 'directv': 0.5822352720828262, 'arlington': 0.5708028473889168, 'mani': 0.5104161884921606, 'is': 0.5208344124328843, 'prison': 0.5907915006550366, 'warwick': 0.5457189027060968, 'rhode': 0.5302638335516788, 'particip': 0.514567011261574, 'network': 0.5329550237159685, 'massiv': 0.5290014337778247, 'video': 0.5640006225794798, 'manhattan': 0.541117636041413, 'chicago': 0.5230349008007742, 'illinoi': 0.5243853618462702, 'offer': 0.5136054796819797, 'interpret': 0.5236846735728028, 'jose': 0.5907915006550366, 'defend': 0.6005785330814327, 'connect': 0.5367019853239534, 'group': 0.5230550408910352, 'march': 0.6041290633285021, 'nine': 0.5243853618462702, 'engag': 0.5194963969702706, 'six': 0.5162074600785497, 'worth': 0.5691047024023227, 'seiz': 0.5634167136141068, 'dure': 0.5173548438880837, 'search': 0.5123272791609339, 'center': 0.5114049913140899, 'februari': 0.7245727319675489, 'justic': 0.5407271652993795, 'depart': 0.5202518222743581, 'top': 0.5152216629439988, 'site': 0.5309524370349152, 'involv': 0.5369818374828016, 'smyrna': 0.5457189027060968, 'tennesse': 0.5278825031720741, 'millennium': 0.5605276671033578, 'dmca': 0.5457189027060968, 'more': 0.5374287886612581, 'bootleg': 0.5914378054121936, 'tape': 0.5580028675556494, 'thai': 0.5457189027060968, 'nation': 0.5190218128691566, 'import': 0.5126215605126814, 'decrypt': 0.541117636041413, 'lead': 0.5141646213874174, 'against': 0.6001748706354888, 'develop': 0.5105169334212459, 'tv': 0.5290014337778247, 'staten': 0.5457189027060968, 'fake': 0.537884843465366, 'loui': 0.526879475912953, 'vuitton': 0.5457189027060968, 'handbag': 0.537884843465366, 'via': 0.5183509926619767, 'label': 0.5754338998110745, 'auction': 0.6001748706354888, 'web': 0.5255491229922816, 'south': 0.5183509926619767, 'carolina': 0.5218641030996454, 'cloth': 0.5259720025756639, 'trademark': 0.5317083568070534, 'januari': 0.6060343549124009, 'most': 0.53189486686825, 'advanc': 0.5136054796819797, 'condit': 0.5132591277333567, 'he': 0.5199183557136621, 'acknowledg': 0.5251446332703582, 'nearli': 0.5194963969702706, 'loss': 0.5167582571943722, 'dish': 0.5333916235451629, 'decemb': 0.6199690721330995, 'pair': 0.537884843465366, 'cupertino': 0.541117636041413, 'econom': 0.5367019853239534, 'espionag': 0.5708028473889168, 'from': 0.5360081446617271, 'silicon': 0.5914378054121936, 'valley': 0.6005785330814327, 'compani': 0.5618202766176718, 'owner': 0.5208344124328843, 'mount': 0.5302638335516788, 'kisco': 0.5536641920562937, 'white': 0.5347096877761675, 'plain': 0.5487707236925404, 'receiv': 0.5110536683071538, 'akron': 0.5457189027060968, 'produc': 0.5137863216936946, 'reproduc': 0.5580028675556494, 'vancouv': 0.541117636041413, 'washington': 0.5127745614961408, 'to': 0.5109416667960991, 'of': 0.5141646213874174, 'corpor': 0.5180040723308635, 'fugit': 0.5457189027060968, 'sold': 0.5213335408598266, 'babi': 0.5278825031720741, 'formula': 0.5333916235451629, 'longest': 0.537884843465366, 'affirm': 0.526879475912953, 'sixth': 0.5354014236944584, 'circuit': 0.5302638335516788, 'appeal': 0.5251446332703582, 'fort': 0.5278825031720741, 'walton': 0.541117636041413, 'beach': 0.5290014337778247, 'member': 0.5127745614961408, 'drinkordi': 0.5457189027060968, 'warez': 0.5914378054121936, 'month': 0.5413589650810839, 'gabriel': 0.5757696869307322, 'conspir': 0.537884843465366, 'traffic': 0.5213335408598266, 'research': 0.5137863216936946, 'fellow': 0.5251446332703582, 'harvard': 0.5317083568070534, 'medic': 0.515947433434125, 'school': 0.5173548438880837, 'return': 0.5141646213874174, 'la': 0.5224297743109482, 'vega': 0.5302638335516788, 'nab': 0.541117636041413, 'bandwidth': 0.537884843465366, 'west': 0.5194963969702706, 'de': 0.5251446332703582, 'moin': 0.541117636041413, 'ioway': 0.5536641920562937, 'woman': 0.5557650063441482, 'program': 0.5102207404588528, 'green': 0.5213335408598266, 'bay': 0.5230349008007742, 'wisconsin': 0.5224297743109482, 'intrus': 0.5457189027060968, 'numer': 0.5180040723308635, 'destruct': 0.5302638335516788, 'iowa': 0.526879475912953, 'employe': 0.5190956713401987, 'placement': 0.526879475912953, 'firm': 0.5224297743109482, 'scientist': 0.5367019853239534, 'provid': 0.5095109064345783, 'fals': 0.526879475912953, 'statement': 0.5149961340166375, 'regard': 0.514567011261574, 'cleveland': 0.5333916235451629, 'clinic': 0.5203635826496897, 'foundat': 0.5224297743109482, 'expand': 0.5170504310324817, 'lucent': 0.5536641920562937, 'accus': 0.5333916235451629, 'pass': 0.5147780498488493, 'chines': 0.5333916235451629, 'vacavil': 0.5457189027060968, 'than': 0.537884843465366, 'wolcott': 0.5536641920562937, 'fabric': 0.5354014236944584, 'metal': 0.5259720025756639, 'product': 0.5139726106344924, 'no': 0.5272109593639595, 'net': 0.5502892665407164, 'unlaw': 0.5667832470903259, 'prosecut': 0.5259720025756639, 'middl': 0.5218641030996454, 'district': 0.5199183557136621, 'one': 0.5101259111371791, 'world': 0.514567011261574, 'sophist': 0.5290014337778247, 'syndic': 0.5667832470903259, 'largest': 0.518714394330629, 'seizur': 0.5302638335516788, 'dvd': 0.5457189027060968, 'unit': 0.5200658640313811, 'histori': 0.5291340225231481, 'russian': 0.5302638335516788, 'enter': 0.5147780498488493, 'agreement': 0.5199183557136621, 'law': 0.5121856808819693, 'enforc': 0.5173548438880837, 'target': 0.5183509926619767, 'intel': 0.5457189027060968, 'santa': 0.526879475912953, 'clara': 0.5536641920562937, 'for': 0.5106197813142533, 'on': 0.5141646213874174, 'liberti': 0.5354014236944584, 'mississippi': 0.5278825031720741, 'biggest': 0.5317083568070534, 'go': 0.5139726106344924, 'home': 0.5183871056987168, 'page': 0.5318593439427601, 'updat': 0.5147780498488493}\n"]}],"source":["# Print the TF-IDF of document with ID GX000-14-10770491\n","print(\"TF-IDF of GX000-14-10770491:\", tf_idf_model.tf_idf_scores['GX000-14-10770491'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XguqwobV9Ex7","outputId":"79dcdd18-a26f-425d-fcae-6f723a60c636"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>GX000-01-10544170</th>\n","      <th>GX000-09-2703409</th>\n","      <th>GX000-10-4524900</th>\n","      <th>GX000-14-10770491</th>\n","      <th>GX000-14-16748010</th>\n","      <th>GX000-14-5445022</th>\n","      <th>GX000-16-0145015</th>\n","      <th>GX000-16-4063715</th>\n","      <th>GX000-21-2440731</th>\n","      <th>GX000-22-12322384</th>\n","      <th>...</th>\n","      <th>GX001-37-11210821</th>\n","      <th>GX001-37-1534868</th>\n","      <th>GX001-37-16595181</th>\n","      <th>GX001-37-2825497</th>\n","      <th>GX001-39-3118899</th>\n","      <th>GX001-41-4669647</th>\n","      <th>GX001-43-0085597</th>\n","      <th>GX001-43-11803200</th>\n","      <th>GX001-44-13913188</th>\n","      <th>GX001-45-9859222</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>link</th>\n","      <td>0.642413</td>\n","      <td>0.506953</td>\n","      <td>0.530517</td>\n","      <td>0.000000</td>\n","      <td>0.507423</td>\n","      <td>0.527465</td>\n","      <td>0.545776</td>\n","      <td>0.525465</td>\n","      <td>0.504629</td>\n","      <td>0.508451</td>\n","      <td>...</td>\n","      <td>0.522269</td>\n","      <td>0.000000</td>\n","      <td>0.518726</td>\n","      <td>0.000000</td>\n","      <td>0.507847</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.517531</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>nation</th>\n","      <td>0.530294</td>\n","      <td>0.510354</td>\n","      <td>0.000000</td>\n","      <td>0.519022</td>\n","      <td>0.522106</td>\n","      <td>0.000000</td>\n","      <td>0.534081</td>\n","      <td>0.570418</td>\n","      <td>0.000000</td>\n","      <td>0.509438</td>\n","      <td>...</td>\n","      <td>0.549739</td>\n","      <td>0.000000</td>\n","      <td>0.588300</td>\n","      <td>0.500764</td>\n","      <td>0.590557</td>\n","      <td>0.508098</td>\n","      <td>0.0</td>\n","      <td>0.504351</td>\n","      <td>0.545441</td>\n","      <td>0.526062</td>\n","    </tr>\n","    <tr>\n","      <th>cancer</th>\n","      <td>1.464480</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.506387</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.584718</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.505408</td>\n","      <td>0.506889</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>institut</th>\n","      <td>0.560303</td>\n","      <td>0.000000</td>\n","      <td>0.560303</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.510783</td>\n","      <td>0.501525</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.595344</td>\n","      <td>0.000000</td>\n","      <td>0.604845</td>\n","      <td>0.501014</td>\n","      <td>0.519383</td>\n","      <td>0.521494</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.548242</td>\n","      <td>0.511972</td>\n","    </tr>\n","    <tr>\n","      <th>center</th>\n","      <td>0.536327</td>\n","      <td>0.531039</td>\n","      <td>0.000000</td>\n","      <td>0.511405</td>\n","      <td>0.506627</td>\n","      <td>0.000000</td>\n","      <td>0.513623</td>\n","      <td>0.558460</td>\n","      <td>0.502755</td>\n","      <td>0.503772</td>\n","      <td>...</td>\n","      <td>0.576213</td>\n","      <td>0.512575</td>\n","      <td>0.527864</td>\n","      <td>0.000000</td>\n","      <td>0.514012</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.521796</td>\n","      <td>0.510818</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 100 columns</p>\n","</div>"],"text/plain":["          GX000-01-10544170  GX000-09-2703409  GX000-10-4524900  \\\n","link               0.642413          0.506953          0.530517   \n","nation             0.530294          0.510354          0.000000   \n","cancer             1.464480          0.000000          0.000000   \n","institut           0.560303          0.000000          0.560303   \n","center             0.536327          0.531039          0.000000   \n","\n","          GX000-14-10770491  GX000-14-16748010  GX000-14-5445022  \\\n","link               0.000000           0.507423          0.527465   \n","nation             0.519022           0.522106          0.000000   \n","cancer             0.000000           0.000000          0.000000   \n","institut           0.000000           0.000000          0.000000   \n","center             0.511405           0.506627          0.000000   \n","\n","          GX000-16-0145015  GX000-16-4063715  GX000-21-2440731  \\\n","link              0.545776          0.525465          0.504629   \n","nation            0.534081          0.570418          0.000000   \n","cancer            0.000000          0.506387          0.000000   \n","institut          0.000000          0.510783          0.501525   \n","center            0.513623          0.558460          0.502755   \n","\n","          GX000-22-12322384  ...  GX001-37-11210821  GX001-37-1534868  \\\n","link               0.508451  ...           0.522269          0.000000   \n","nation             0.509438  ...           0.549739          0.000000   \n","cancer             0.000000  ...           0.584718          0.000000   \n","institut           0.000000  ...           0.595344          0.000000   \n","center             0.503772  ...           0.576213          0.512575   \n","\n","          GX001-37-16595181  GX001-37-2825497  GX001-39-3118899  \\\n","link               0.518726          0.000000          0.507847   \n","nation             0.588300          0.500764          0.590557   \n","cancer             0.000000          0.505408          0.506889   \n","institut           0.604845          0.501014          0.519383   \n","center             0.527864          0.000000          0.514012   \n","\n","          GX001-41-4669647  GX001-43-0085597  GX001-43-11803200  \\\n","link              0.000000               0.0           0.517531   \n","nation            0.508098               0.0           0.504351   \n","cancer            0.000000               0.0           0.000000   \n","institut          0.521494               0.0           0.000000   \n","center            0.000000               0.0           0.000000   \n","\n","          GX001-44-13913188  GX001-45-9859222  \n","link               0.000000          0.000000  \n","nation             0.545441          0.526062  \n","cancer             0.000000          0.000000  \n","institut           0.548242          0.511972  \n","center             0.521796          0.510818  \n","\n","[5 rows x 100 columns]"]},"execution_count":251,"metadata":{},"output_type":"execute_result"}],"source":["# Create a dataframe to store the TF-IDF with the document id as the index\n","tf_idf_df = tf_idf_model.get_tf_idf_dataframe()\n","\n","# Fill NaN values with 0\n","tf_idf_df = tf_idf_df.fillna(0)\n","tf_idf_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCAeQ1RM9Ex7"},"outputs":[],"source":["# Store the DataFrame to disk\n","tf_idf_df.to_csv('./TF_IDF_Q3_Part1.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbW8RGGm9Ex8"},"outputs":[],"source":["# Implement Learning by Prototypes Classifier where class prototype is the mean of the training examples of that class \n","# To classify a test image, find the prototype with the shortest distance.\n","from collections import OrderedDict\n","\n","class Rocchio:\n","\n","    # Constructor Function for our class\n","    def __init__(self):\n","        \n","        # class prototypes\n","        self.class_prototype = OrderedDict()  \n","        \n","        # label frequency array\n","        self.class_count = OrderedDict()  \n","        \n","        # labels\n","        self.class_name = []  \n","\n","    # Training Function for our class\n","    def fit(self, X, y):\n","\n","        # Loop over all the input feature vectors\n","        for i in range(len(y)):\n","\n","            if y[i] not in self.class_name:\n","                \n","                # Encountering a new label for the first time\n","                self.class_name.append(y[i])\n","                self.class_prototype[y[i]] = np.zeros(len(X[0]))\n","                self.class_count[y[i]] = 0\n","            \n","            # Steps for the mean calculation for this class's prototype\n","            self.class_prototype[y[i]] += X[i]\n","            self.class_count[y[i]] += 1\n","\n","        # Looping over the prototype dictionary to calculate mean\n","        for key in self.class_prototype:\n","            self.class_prototype[key] = self.class_prototype[key]/self.class_count[key]\n","    \n","    # Function to predict labels for multiple feature vectors based on Eucledian Distance\n","    def predict_E(self, X):\n","        \n","        # Predictions array\n","        pred = []\n","\n","        # Looping over all feature vectors\n","        for i in range(len(X)):\n","\n","            # Distance array\n","            dist = []\n","            \n","            # Calculating the distance of our feature vector from all the prototypes\n","            for key in self.class_prototype:\n","                dist.append(np.linalg.norm(X[i] - self.class_prototype[key]))\n","\n","            # Taking the label of the prototype at the minimum distance\n","            pred.append(self.class_name[dist.index(min(dist))])\n","        \n","        # Returning the predictions array\n","        return np.array(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AnuBSXPT9Ex8"},"outputs":[],"source":["# Implement the KNN algorithm classifier without using the sklearn library\n","class KNNClassifier:\n","\n","  # Constructor Function for our class\n","    def __init__(self, k = 5):\n","\n","        # by default k = 5\n","        self.k = k  \n","        \n","        # class prototypes\n","        self.X_train = None \n","        \n","        # labels\n","        self.y_train = None \n","        self.isFirst = True\n","        self.pre_dis = dict()\n","    \n","\n","    # Training Function \n","    def fit(self, X, y):\n","\n","        # make sure that X is a 2D array and Y is a 1D array else raise an error \n","        if len(X.shape) != 2 and len(y.shape) != 1:\n","            raise Exception(\"Error: X must be a 2D array\")\n","\n","        self.X_train = X\n","        self.y_train = y\n","    \n","    # Optimized predict function for HyperTuning using a fixed validation set\n","    def hyper_tuning_predict(self, X, method='U'):\n","      \n","        # Method parameter decides if we want to do a weighted (W) prediction or an unweighted (U) prediction\n","        # By default, we do an unweighted prediction\n","        if len(X.shape) != 2:\n","            raise Exception(\"Error: X must be a 2D array\")\n","        \n","        if str(method).lower() != 'u' and str(method).lower() != 'w':\n","            raise Exception(\"Error: Please select a valid method : (U/W)\")\n","        \n","        y_pred = []\n","\n","        if str(method).lower() == 'u':\n","          \n","            for i in range(len(X)):\n","              \n","                # predicting the labels of all the feature vector one by one\n","                label = self.hyper_tuning_predict_one(X[i], i)\n","                y_pred.append(label)\n","                \n","        else:\n","          \n","            for i in range(len(X)):\n","              \n","                # predicting the labels of all the feature vector one by one\n","                label = self.hyper_tuning_predict_one_Weighted(X[i], i)\n","                y_pred.append(label)\n","                \n","        self.isFirst = False\n","        return y_pred\n","\n","    # Unweighted Single Prediction Method for HyperTuning\n","    def hyper_tuning_predict_one(self, row, j):\n","\n","        if self.isFirst:\n","          \n","            for i in range(len(self.X_train)):\n","              \n","                dist = self.distance(row, self.X_train[i])\n","                \n","                if self.pre_dis.get(j) != None:\n","                    self.pre_dis[j].append((dist, self.y_train[i]))\n","                    \n","                else:\n","                    self.pre_dis[j] = [(dist, self.y_train[i])]\n","\n","            # Sorting the distance array \n","            self.pre_dis[j].sort()\n","\n","        neighbors = self.pre_dis[j][:self.k]\n","\n","        weights = dict()\n","        freq = dict()\n","        maxfreq = 0\n","        \n","        # Calculating the Weights\n","        for i in range(len(neighbors)):\n","          \n","            label = neighbors[i][1]\n","            \n","            if weights.get(label) is not None:\n","                weights[label] += neighbors[i][0]\n","                freq[label] += 1\n","                \n","            else:\n","                weights[label] = neighbors[i][0]\n","                freq[label] = 1\n","            \n","            maxfreq = max(maxfreq, freq[label])\n","        \n","        minWeight = -1\n","        answer_label = 0\n","\n","        # Finding the label with minimum weight.\n","        # Weight of a label is basically the sum of all distances from datapoints of that label\n","        # This significantly decreases the chances of a Tie.\n","        for key, val in weights.items():\n","            if (minWeight == -1 or minWeight > val) and freq[key] == maxfreq:\n","                answer_label = key\n","                minWeight = val\n","\n","        return answer_label\n","    \n","    # Weighted Single Prediction method for hyperTuning\n","    def hyper_tuning_predict_one_Weighted(self, row, j):\n","      \n","        if self.isFirst:\n","          \n","            for i in range(len(self.X_train)):\n","              \n","              dist = self.distance(row, self.X_train[i])\n","              \n","              if self.pre_dis.get(j) != None:\n","                  self.pre_dis[j].append((dist, self.y_train[i]))\n","                  \n","              else:\n","                  self.pre_dis[j] = [(dist, self.y_train[i])]\n","            \n","            # Sorting the distance array \n","            self.pre_dis[j].sort()\n","\n","        neighbors = self.pre_dis[j][:self.k]\n","\n","        # Weight of neighbors\n","        weights = dict()\n","\n","        # Calculating the Weights\n","        for i in range(len(neighbors)):\n","          \n","            label = neighbors[i][1]\n","            \n","            if weights.get(label) is not None:\n","                weights[label] += neighbors[i][0]\n","              \n","            else:\n","                weights[label] = neighbors[i][0]\n","        \n","        minWeight = -1\n","        answer_label = 0\n","\n","        # Finding the label with minimum weight.\n","        # Weight of a label is basically the some of all distances from datapoints of that label\n","        # This significantly decreases the chances of a Tie.\n","        for key, val in weights.items():\n","            if minWeight == -1 or minWeight > val:\n","                answer_label = key\n","                minWeight = val\n","\n","        return answer_label\n","\n","    # Function to predict the labels of array of Image\n","    def predict(self, X, method='U'):\n","      \n","        # Method parameter decides if we want to do a weighted (W) prediction or an unweighted (U) prediction\n","        # By default, we do an unweighted prediction\n","        if str(method).lower() not in ['u', 'w']:\n","            raise Exception(\"Error: Please select a valid method : (U/W)\")\n","\n","        # make sure that X is a 2D array else return an error\n","        if len(X.shape) != 2:\n","            raise Exception(\"Error: X must be a 2D array\")\n","\n","        # Predictions array\n","        y_pred = []\n","\n","        if str(method).lower() == 'u':\n","          \n","          for row in X:\n","            \n","              # predicting the labels of all the feature vector one by one\n","              label = self.predict_one(row)\n","              y_pred.append(label)\n","              \n","        else:\n","          \n","          for row in X:\n","            \n","              # predicting the labels of all the feature vector one by one\n","              label = self.predict_one_Weighted(row)\n","              y_pred.append(label)\n","        \n","        return y_pred\n","    \n","    # Predict the label of a single Image\n","    def predict_one(self, row):\n","        \n","        # Distance array\n","        distances = []\n","\n","        # Looping over the feature vector\n","        for i in range(len(self.X_train)):\n","            dist = self.distance(row, self.X_train[i])\n","            distances.append((dist, self.y_train[i]))\n","        \n","        # Sorting the distance array \n","        distances.sort()\n","\n","        # Extracting first k neighbors\n","        neighbors = distances[:self.k]\n","\n","        weights = dict()\n","        freq = dict()\n","        maxfreq = 0\n","        \n","        # Calculating the Weights\n","        for i in range(len(neighbors)):\n","          \n","            label = neighbors[i][1]\n","            \n","            if weights.get(label) is not None:\n","              \n","                weights[label] += neighbors[i][0]\n","                freq[label] += 1\n","              \n","            else:\n","              \n","                weights[label] = neighbors[i][0]\n","                freq[label] = 1\n","            \n","            maxfreq = max(maxfreq, freq[label])\n","        \n","        minWeight = -1\n","        answer_label = 0\n","\n","        # Finding the label with minimum weight.\n","        # Weight of a label is basically the sum of all distances from datapoints of that label\n","        # This significantly decreases the chances of a Tie.\n","        for key, val in weights.items():\n","            if (minWeight == -1 or minWeight > val) and freq[key] == maxfreq:\n","                answer_label = key\n","                minWeight = val\n","\n","        return answer_label\n","    \n","    # Precit the label of a feature vector based on weighted KNN\n","    def predict_one_Weighted(self, row):\n","        \n","        # Distance array\n","        distances = []\n","\n","        # Looping over the feature vector\n","        for i in range(len(self.X_train)):\n","            dist = self.distance(row, self.X_train[i])\n","            distances.append((dist, self.y_train[i]))\n","        \n","        # Sorting the distance array \n","        distances.sort()\n","\n","        # Extracting first k neighbors\n","        neighbors = distances[:self.k]\n","\n","        # Weight of neighbors\n","        weights = dict()\n","        \n","        # Calculating the Weights\n","        for i in range(len(neighbors)):\n","          \n","            label = neighbors[i][1]\n","            \n","            if weights.get(label) is not None:\n","                weights[label] += neighbors[i][0]\n","                \n","            else:\n","                weights[label] = neighbors[i][0]\n","        \n","        minWeight = -1\n","        answer_label = 0\n","\n","        # Finding the label with minimum weight.\n","        # Weight of a label is basically the sum of all distances from datapoints of that label\n","        # This significantly decreases the chances of a Tie.\n","        for key, val in weights.items():\n","            if minWeight == -1 or minWeight > val:\n","                answer_label = key\n","                minWeight = val\n","\n","        return answer_label\n","    \n","    # Function to find Euclidian distance between two feature vector\n","    def distance(self, a, b):\n","        return np.linalg.norm(a - b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rB2p9-F9EyA"},"outputs":[],"source":["xtrain = []\n","ytrain = []\n","for doc in train_set:\n","    if doc in tf_idf_df.keys():\n","        xtrain.append(tf_idf_df[doc])\n","        ytrain.append(inv_classes_dict[doc])\n","        \n","xtest = []\n","ytest = []\n","for doc in test_set:\n","    if doc in tf_idf_df.keys():\n","        xtest.append(tf_idf_df[doc])\n","        ytest.append(inv_classes_dict[doc])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ySRj_5G9EyC","outputId":"b6aa53e5-f20b-4023-d1e2-66d989b50e5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         701       0.00      1.00      0.00         0\n","         704       0.00      1.00      0.00         0\n","         711       0.00      1.00      0.00         0\n","         718       0.00      1.00      0.00         0\n","         724       0.00      1.00      0.00         0\n","         725       1.00      0.00      0.00         1\n","         729       0.00      1.00      0.00         0\n","         733       0.00      1.00      0.00         0\n","         736       0.00      1.00      0.00         0\n","         738       0.00      1.00      0.00         0\n","         740       1.00      0.11      0.20         9\n","         743       0.00      1.00      0.00         0\n","         744       1.00      0.50      0.67         4\n","         748       0.00      1.00      0.00         0\n","         749       0.00      1.00      0.00         0\n","         758       0.00      1.00      0.00         0\n","         765       0.00      1.00      0.00         0\n","         772       1.00      0.00      0.00         9\n","         777       0.00      1.00      0.00         0\n","         780       0.00      1.00      0.00         0\n","         800       0.00      1.00      0.00         0\n","         802       0.00      1.00      0.00         0\n","         803       0.00      1.00      0.00         0\n","         823       1.00      0.00      0.00         3\n","         833       0.00      1.00      0.00         0\n","         840       0.00      1.00      0.00         0\n","         849       1.00      0.00      0.00         2\n","\n","    accuracy                           0.11        28\n","   macro avg       0.22      0.80      0.03        28\n","weighted avg       1.00      0.11      0.16        28\n","\n"]}],"source":["rocchio = Rocchio()\n","rocchio.fit(xtrain, ytrain)\n","prediction = rocchio.predict_E(xtest)\n","\n","report = classification_report(prediction, ytest, zero_division=True)\n","print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_vS-9EN9EyC","outputId":"f3237b9e-9282-4949-f25e-0f1df9b4b2a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         701       0.00      1.00      0.00         0\n","         704       0.00      1.00      0.00         0\n","         711       0.00      1.00      0.00         0\n","         718       0.00      1.00      0.00         0\n","         724       0.00      1.00      0.00         0\n","         725       1.00      0.00      0.00         1\n","         728       1.00      0.00      0.00         1\n","         729       0.00      1.00      0.00         0\n","         733       0.00      1.00      0.00         0\n","         736       0.00      1.00      0.00         0\n","         738       0.00      1.00      0.00         0\n","         740       0.00      0.00      0.00         2\n","         742       1.00      0.00      0.00         1\n","         743       0.00      1.00      0.00         0\n","         744       0.50      1.00      0.67         1\n","         748       1.00      0.25      0.40         4\n","         749       0.00      1.00      0.00         0\n","         758       0.00      1.00      0.00         0\n","         765       0.00      1.00      0.00         0\n","         772       1.00      0.00      0.00         7\n","         777       0.00      1.00      0.00         0\n","         780       0.00      1.00      0.00         0\n","         794       1.00      0.00      0.00         1\n","         800       0.00      1.00      0.00         0\n","         802       0.00      1.00      0.00         0\n","         803       0.00      1.00      0.00         0\n","         807       1.00      0.00      0.00         1\n","         823       1.00      0.00      0.00         6\n","         833       0.00      1.00      0.00         0\n","         840       0.00      1.00      0.00         0\n","         849       1.00      0.00      0.00         3\n","\n","    accuracy                           0.07        28\n","   macro avg       0.31      0.69      0.03        28\n","weighted avg       0.91      0.07      0.08        28\n","\n"]}],"source":["knn_1 = KNNClassifier(1)\n","knn_1.fit(np.array(xtrain), np.array(ytrain))\n","prediction = knn_1.predict(np.array(xtest))\n","report = classification_report(prediction, ytest, zero_division=True)\n","print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BiAXgSJ49EyD","outputId":"1c1e20ff-a75a-4ba1-e3d1-b3c383050e1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         701       0.00      1.00      0.00         0\n","         704       0.00      1.00      0.00         0\n","         711       0.00      1.00      0.00         0\n","         718       0.00      1.00      0.00         0\n","         724       0.00      1.00      0.00         0\n","         725       1.00      0.00      0.00         1\n","         728       1.00      0.00      0.00         1\n","         729       0.00      1.00      0.00         0\n","         733       0.00      1.00      0.00         0\n","         736       0.00      1.00      0.00         0\n","         738       0.00      1.00      0.00         0\n","         740       0.00      0.00      0.00         2\n","         742       1.00      0.00      0.00         1\n","         743       0.00      1.00      0.00         0\n","         744       0.50      1.00      0.67         1\n","         748       1.00      0.25      0.40         4\n","         749       0.00      1.00      0.00         0\n","         758       0.00      1.00      0.00         0\n","         765       0.00      1.00      0.00         0\n","         772       1.00      0.00      0.00         7\n","         777       0.00      1.00      0.00         0\n","         780       0.00      1.00      0.00         0\n","         794       1.00      0.00      0.00         1\n","         800       0.00      1.00      0.00         0\n","         802       0.00      1.00      0.00         0\n","         803       0.00      1.00      0.00         0\n","         823       1.00      0.00      0.00         7\n","         833       0.00      1.00      0.00         0\n","         840       0.00      1.00      0.00         0\n","         849       1.00      0.00      0.00         3\n","\n","    accuracy                           0.07        28\n","   macro avg       0.28      0.71      0.04        28\n","weighted avg       0.91      0.07      0.08        28\n","\n"]}],"source":["knn_3 = KNNClassifier(3)\n","knn_3.fit(np.array(xtrain), np.array(ytrain))\n","prediction = knn_3.predict(np.array(xtest))\n","report = classification_report(prediction, ytest, zero_division=True)\n","print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioOHJ4B79EyD","outputId":"c42403b8-45e0-4a08-ab1e-dab870b17f03"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         701       0.00      1.00      0.00         0\n","         704       0.00      1.00      0.00         0\n","         711       0.00      1.00      0.00         0\n","         718       0.00      1.00      0.00         0\n","         724       0.00      1.00      0.00         0\n","         725       1.00      0.00      0.00         1\n","         729       0.00      1.00      0.00         0\n","         733       0.00      1.00      0.00         0\n","         736       0.00      1.00      0.00         0\n","         738       0.00      1.00      0.00         0\n","         740       0.00      0.00      0.00         2\n","         742       1.00      0.00      0.00         1\n","         743       0.00      1.00      0.00         0\n","         744       0.50      1.00      0.67         1\n","         748       0.00      0.00      0.00         2\n","         749       0.00      1.00      0.00         0\n","         758       0.00      1.00      0.00         0\n","         765       0.00      1.00      0.00         0\n","         772       1.00      0.00      0.00         3\n","         777       0.00      1.00      0.00         0\n","         780       0.00      1.00      0.00         0\n","         794       1.00      0.00      0.00         1\n","         800       0.00      1.00      0.00         0\n","         802       0.00      1.00      0.00         0\n","         803       0.00      1.00      0.00         0\n","         823       1.00      0.00      0.00        15\n","         833       0.00      1.00      0.00         0\n","         840       0.00      1.00      0.00         0\n","         849       1.00      0.00      0.00         2\n","\n","    accuracy                           0.04        28\n","   macro avg       0.22      0.72      0.02        28\n","weighted avg       0.84      0.04      0.02        28\n","\n"]}],"source":["knn_5 = KNNClassifier(5)\n","knn_5.fit(np.array(xtrain), np.array(ytrain))\n","prediction = knn_5.predict(np.array(xtest))\n","report = classification_report(prediction, ytest, zero_division=True)\n","print(report)"]}],"metadata":{"colab":{"name":"IR_Assignment_3_4.ipynb","provenance":[]},"interpreter":{"hash":"369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"},"kernelspec":{"display_name":"Python 3.10.1 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
