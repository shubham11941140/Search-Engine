There is a CSV file named **"Description.csv"** that gives a detailed description of the query, File name, Doc ID and Wikipedia Link.

This helps map the docid in the posting lists to the 60 text files in the folder.

There are 5 Python files. Ensure that you keep all the files in the same folder. Multiple files use functions from other files
which are imported. If the folder is not same, we cannot import the functions or add the necessary path to the files to import.

1) **index_constructions.py** -- This file on running will extract the Wikipedia links, write it onto text files and create the inverted index.

This inverted index is stored onto the JSON file - "inverted_index.json". You can preview the file and see a dictionary of the inverted index
(key: word, value: list of docids).

The output of the code execution is in **"1.PNG"** (The output is the posting list of 3 most commonly used terms and 3 least commonly used terms).
(The inverted index is also stored in the JSON file - "inverted_index.json") (The 60 text files are also generated according to the specified naming convention)



2) **merging.py** -- This file will take input queries and output the merged posting lists for the queries. It has been extended to support queries
with Multiple words.

We have an infinite loop that can be used to input queries spaced by " AND " (Eg: "word1 AND word2 AND word3") which will output the merged posting lists.
It can be terminated with the "exit" command.

The output of the code execution is in **"2.PNG"** (The output is a list of docids matching each query)
(Few queries are not found in the index, so empty list is returned)



3) **add_skip_pointers.py** -- This file will add skip pointers to the inverted index. It will read the inverted index from the JSON file and then create a new
dictionary where each posting list will have an extra field to denote the skip pointers.

This new dictionary (inverted index) will be stored onto the JSON file - "inverted_index_with_skip_pointers.json".

The output of the code execution is in **"3.PNG"** (Comparison of the average code execution time in 100 iterations)



Note: The execution time with skip pointers is more than without skip pointers as to check the skip pointer conditions the code is taking more time.
The usage of skip lists will be to reduce the traversal but the list is itself very small, so it makes a difference of time in nanoseconds. For a small list,
it adds overhead of comparison thus slowing down the code.

4) **scoring_tf_idf.py** -- This file will take the retrieved posting lists of the given queries and then calculate the TF-IDF score.

The most simple TF-IDF scheme is used with the formula:

The TF-IDF scheme used is the **Vector Space Model (VSM).**

**The TF is calculated as: TF = (Number of times term occurs in document) / (Total number of terms in document)**

**The IDF is calculated as: IDF = Logarithm ((Total Number of Documents) / (Number of documents containing the term))**

The output of the code execution is in **"4.PNG"** (List of docids mathcing the queries as specified in a sorted order)
(Comparison of the average code execution time in 100 iterations for each query)



5) **write_csv_with_links_documents.py** -- This file will write the CSV file with the links and docids. The CSV file mentioned at the beginning to give a description of the query, file name, doc ID and Wikipedia Link is generated by this file.

The output of the code execution is in **"Description.csv" (The CSV file is generated)**
